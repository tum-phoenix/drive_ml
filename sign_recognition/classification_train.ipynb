{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous net saved in _'01-07-17_convnet.h5'_ , gets a 97% accuracy rate on the test dataset (!) of GTSRB. However, when it must predict on images that don't belong to the GTSRB dataset, it sees traffic signs where there are none, and with very high (~100%) confidence. This is a problem because during a race, the car won't see any traffic signs most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach to fix this, is to add a _background_ class, also called _Zero_ class, as the 43th class to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Picture data functions + Load Numpy and Plot library\n",
    "from utilities.gtsrb_loader.load_data import load_data\n",
    "from utilities.gtsrb_loader.get_folderpath import get_folderpath\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTSRB test dataset is in place, you're fine.\n",
      "/data_on_server/Images/GTSRB/Final_Training/Images\n"
     ]
    }
   ],
   "source": [
    "# Load Trainingsdata (could take 1 or 2 minutes to load data) / X --> images, y --> labels\n",
    "path = get_folderpath(subset='train', original_images=True)\n",
    "X_train, y_train = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Load zero class (Pictures with no traffic signs on them)\n",
    "path_zero = \"C:\\\\Users\\\\tomas\\\\Desktop\\\\GTSRB\\\\Zero_Class\\\\ 00043\"\n",
    "X_train_zero, y_train_zero = load_data(path_zero)\n",
    "print (X_train_zero) #Frage: X_train_zero ist leer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images to 64 x 64\n",
    "from skimage.transform import resize\n",
    "X_train_zero = np.array([resize(pic, (64, 64), mode='edge') for pic in X_train_zero])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this second net, we want to distinguish between _background_ and _traffic sign_ reliably. Thats why I dont balance number of training images per class yet. Any traffic sign does the job. Since our zero class is only in gray color, we will transfor the GTSRB data to grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black and White Zero class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Matrix x: (Anzahl Zeilen, Anzahl Spalten) (36, 27)\n"
     ]
    }
   ],
   "source": [
    "#Generate Black and white pictures (First just for class 29 to test)\n",
    "from skimage import color\n",
    "x = color.rgb2gray(X_train[0])\n",
    "print (y_train[0])\n",
    "print (\"Matrix x: (Anzahl Zeilen, Anzahl Spalten)\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADtCAYAAACvfY5sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XuQ1eWZJ/Dvc/pGN81VmuZOIxKR\niOLQ0USN8TKZZUyyOomxkq1kk63sOpNNanPdGs0/mdp1pjK1MyZblWQqzMbV3VyMYzSyMRfwEolG\nVMAbAgZBLg0NDXLpbujLuTz7Rx92kff70ud0nz59zo/vp8oSHt7+nd85/fL24TzP+z7m7hARkeqX\nGu8bEBGR0tCCLiKSEFrQRUQSQgu6iEhCaEEXEUkILegiIgmhBV1EJCG0oIuIJIQWdBGRhKgdzReb\n2SoA/x1ADYD/4e7fOtf4GTNmeFtb22gesvJ1nwxCnSd66dDB2hoa7z8Zjq8xo2NTkXgux2J8V7CD\nDM7/ydnq6/mUmVTL3xvMmNgcxPrSWTq2p7+HxlO5gfC602cEsd2HjuDIiR7+ghRJczt04sSJINbV\n1UXHplJ8PnR3dxc81qJzO5yvLAYAxeyEnzBhQlHxadOmBbH+/n46treXrwHsvmfNmhXEurq60N3d\nPezcHvGCbmY1AL4H4IMAOgC8aGZr3H1r7Gva2tqwcePGkT5kdfjNc0Ho7t/+gQ7d0zKFxrc//0wQ\nu6C2no6tq22g8f6+cNE8SWIAkMn10bh7uJAuWDidjv3TmU00/u9WXhPEXj0U/qUGgPVv/I7GG/p2\nBbH/8PF/H8Ta/9M36dcXS3Ob++UvfxnEvvvd79Kxzc3hD3IAWLt2bRBrauJzp76ez/mTJ8M3TT09\n/M1AJpOhcbbQx34gX3LJJTT+0Y9+NIjt3LmTjn366adpvK8v/Lt31113BbGvfvWr9OvPNpqPXK4E\n8Ka773L3QQAPALhlFNcTqRSa21KVRrOgzwWw74zfd+Rj72Bmd5jZRjPbePjw4VE8nEjZaG5LVRrz\npKi7r3b3dndvb2lpGeuHEykbzW2pNKNZ0PcDmH/G7+flYyLVTnNbqtJoqlxeBLDEzBZhaLJ/AsC/\nKcldVbNV7wtCd1wSxgDg+b2v0viljWHScPemN+hYPxVJ+gyGCc3GBp5AbYxU2zTPmhnEJs3mU+am\nDyylcVy/KgilfsITRBlSIQQATY11YfCiy8JYQyO/h+JpbhMf/vCHg9jSpfz7/tJLL9H4jBlhdVIx\nCUMAqKkJ52traysdG0uszpkzJ4jNnz+fjARuv/12Gr/11luD2Pe//306llUIATwhfPHFFwexWKXN\n2Ua8oLt7xsy+COC3GCrtutfdXx/p9UQqhea2VKtR1aG7+68A/KpE9yJSMTS3pRppp6iISEJoQRcR\nSYhRfeRStIFTwI7NQXgHiS1Z8i5+jSVhwmAI25k2sfB7G0MzF/L4RxaSxB4AvD+MH1r3OB268fEn\nadyPHgtiN193PR2bOtVJ42+R+KL3Lqdj8b5IHOFW6NoZk+nIZuM7WWvZsQLdh8JYlieIy6G/vx/b\nt28P4mz36MqVK+k1WDIM4FvgY9viy+2iiy4qKv7xj388iD366KN07M9+9jMaP3LkSBD7xCc+QcfG\nttyzfQM33XQTHXvttdfSONttOnNmWEgAALW1fKmtqwsT/sePHw9i2Sz/u3E2vUMXEUkILegiIgmh\nBV1EJCG0oIuIJIQWdBGRhChrlUsml8GRgTBD/eyGnwexl57jP2tu/69/F7l6eCh8pVS5lELrB3mm\nvX79L2h80vxwq3DqMr49Gpe30/Dx/3ZnENv1Kj9VcGpdWFUDANPbPxbEBvr5eeipSJVLg4XVBNke\n0lghm6ZfXw7ZbJZu716zZk0QY+eKA8CPfvQjGmcNICqlyqUUbrmFn0z8yCOP0PjixYuD2OWXX07H\nrlixgsa//vWvB7Fnngn7EADx5hnXX399EGNntQPxJh6s+oXNI1W5iIicZ7Sgi4gkhBZ0EZGE0IIu\nIpIQWtBFRBKirFUuA5kc3jwUnq3whw1hV/evfZofKh//GZScrD+1/vc03BBp6vDe624Mg5e/l197\nz4s0fDwTVpdc0cLPYUE7P7sDOBVEBgd4JUBtKnw8AGgg39pcT3hGjOf415fDwMAA3nrrrSD+2GOP\nBbG77767HLdUNZ58kp9H1NjI5/Ztt90WxGLn4+zcuZPGWfOMhQv5oUvXXHMNjbOzXE6dCuc7wJty\nxOLd3WEVmKpcRETOM1rQRUQSQgu6iEhCaEEXEUmIUSVFzWw3gB4AWQAZd+d7yPOy6Sx6SVJ0prcE\nsYHtHbFHLfo+E+G6D9BwesN6Pn4W62Ae6Ry+cAkNp2rDjuldbx+gY2ciPJR/SJgkSg/yxBHrYzEU\nD7/nvT1hUiuX5Vu0R6LYuZ1Op7F///4gzrq1v/baa6W5yYS48UaSwEe88cWcOXMKvvaiRYtonDWW\nYN8/gCc/Y/H+/jBZDxSXFD12LDxGo9CkaCmqXG5w9/CAFpHqp7ktVUUfuYiIJMRoF3QHsNbMNpnZ\nHWyAmd1hZhvNbOOJnp5RPpxI2RQ1t2On7ImU02g/crnW3feb2UwA68xsu7u/40Ndd18NYDUAvOvC\nReO380OkOEXN7fnz52tuy7gb1Tt0d9+f/38XgEcAXFmKmxIZb5rbUo1G/A7dzCYCSLl7T/7Xfwbg\nv5zzwcwwjWSXW0i2eElNWGExpCF29XPeb2WKVWWEGW1/ZTMf2RB5nYopBurYS8M1deHP+5mzWfUM\nADRF4mEWP17lwl+PGnIkQHowrHJxL02Vy0jmdiqVQlNT+BqwaoopU6bEHreoeFJs2rSJxuvr+dwu\n5vXYu5fPbfZ9iVXEFPN4sSqXWIMLVuXCrhFrsnG20ayCrQAeyT/ZWgA/cfffjOJ6IpVCc1uq0ogX\ndHffBYD3fRKpYprbUq1UtigikhBa0EVEEqK8mUQzGElGpElS1Gtiyc9YnG+trWx8O+/RZ58KYo1H\n+YbFha0zaTxNEjnhK583L7KVmiQpj50Ij24AgGlFJEUzmcj26EhSNGfh69RNEqvZEiVFR8LMaBKP\nbdlmnd5PXyPpHn/88SDGtroD8SRlMRYsWEDjLBl5+PBhOraYZPXg4GDBjxfD9jQUmhTVO3QRkYTQ\ngi4ikhBa0EVEEkILuohIQmhBFxFJiLJWuWQyGXQdPRrEeZVLNVatFGnvLhre/mzYBf3qubwj+aK5\nPItfN3FS4fexnzcTSdWEWfxpF8yIXCR2VEP4niGbGaAjDfx8q6yF1zhFsv65cTweK51O48CBsPkH\nq04opuKhWu3evZvGH3vssSC2fPlyOvaiiy6i8ebm5oLvY9++fTTOvgfFNM6IGRjgc7sY6XQ6iMWa\nbJxN79BFRBJCC7qISEJoQRcRSQgt6CIiCaEFXUQkIcpa5ZLOZtFFzm1gGdyG86ASAAvm0nBN79tB\nbP923rOyZTavfsHsIjL2c/nYFPkWHD3B+8JOj54UQ867GOANLlKRKpcUqXLhWf/xK3NJp9Po7OwM\n4qzKhTVXSJqFC/m87CF9hTdv5s1bYlUn8+bNK/g+5s/nDVnYeTrFnuXCxBpcxK7BGl8UWtFCrzfi\nrxQRkYqiBV1EJCG0oIuIJIQWdBGRhBg2KWpm9wL4MIAud780H5sO4GcA2gDsBnC7u/NT6s/gALIk\nN8C6tdeOY7OCslkfbvEHgJqT4fEIc2fN4teYG/mZfKorjDVFEqUH+NZ/I0nR6TNa+TXQGImHF0kP\n9tGRDbU8EW6pMEnEtljnikwmlXJux4wmwVXNnnySz+3e3rBBSixxGUt+smvEjgPo6OBzm239nzuX\nFymUIikaS4Sz+zh1KiwaKGWDi/sArDordieAJ9x9CYAn8r8XqTb3QXNbEmTYBd3d1wM4+y3jLQDu\nz//6fgC3lvi+RMac5rYkzUg/Q29199NFtwcBxP4dDjO7w8w2mtnGXtIrT6TCjGhu9/Xxj5FEymnU\nSVEf+pAw+kGhu69293Z3b2+eOHG0DydSNsXM7cbGWA5BpHxGuqAfMrPZAJD/P8nAiVQlzW2pWiPd\n+r8GwGcAfCv//0cL+aIaAJNSYca4FmEGNxt5Y1Qzjtu7S23rlldo/F3LFgexl5//HR0792X+LWy5\n+N2F38ic2TTsngliJ3r4tv0p0SoXsrU5N0hH1tfy9xf9JMN/ajC8Rq40HS5GNLfNDA0NDTR+tljl\nSyxeTJVFpXj22Wdp/Kqrrgpi69ato2NbW/mnXcuWLSv4PmKVK6xq5ChpvgMU9/pns1kar6/nDWDY\nePbxXcmqXMzspwCeA3CxmXWY2ecwNNk/aGY7APxp/vciVUVzW5Jm2Hfo7v7JyB/dVOJ7ESkrzW1J\nGu0UFRFJCC3oIiIJoQVdRCQhytrgwtxRP5gO4vWkgUFNfaxqItYcoPp+Ni1bupz/QWP4Gq2o4c/7\nqVd4pcwNy98kD8jPzEDHfhp2D7P7U6bG9tlMisRZlUskYx+pUsmS4RnSOGM865/cHel0+H1jZ3Ww\nahigOqtZYlg1CwBMmDChoBgArF+/nsavvvrqIHbZZZfRsfv27aNxVlE0c+ZMOraY70usGqXQKhVA\nDS5ERARa0EVEEkMLuohIQmhBFxFJiLImRd0dA2TLdoblAIpOivLmCJXgyON8a/PgW3tpfM67Lwyv\n0dnNr3GEH6iPCZMLuzkAmDEj8gdhMuhUP0/YNEWTomGiMJMJjxQAzrElniTN0xWYFGXNDVgyLJYU\njankZOmaNWtofMeOHTTe3t4exPbs2UPHHj58mMaLOQhtRnRuh05GToMt5vUfJOsbEJ/bqRQpGiBj\nC02U6h26iEhCaEEXEUkILegiIgmhBV1EJCG0oIuIJERZq1yyuRx6esNMMq1yqY1lsmNt7GLVL2W2\n5fkgtOmJn9Kh/2plmPEfEh56P715Kh05eCTSy/JFciTAhSv52M5OGmbJ/abmKfwaiFVuhM8lm+WV\nABapU2FVLkYaBhhpnlIumUyGNkhgVS51dXyusoqHSvIKOWbigQceoGNvvvnmgq87ffp0Gu/q4s2i\nNmzYEMSWLFlCxx44cIDG2Ws9dSr/OxarcmGVJ+z4h3Nh12bzo9BKm8qeQSIiUjAt6CIiCaEFXUQk\nIbSgi4gkRCFNou81sy4z23JG7G/MbL+ZvZz/r/AMiEiF0NyWpCmkyuU+AN8F8L/Oin/b3f+hmAfL\nZrM43tsbxNPsnILapshVYvEKUR9muSfXH+djp0Ve/kx4JkhqfhsdOrWZH8q/89kXgtji96/ijzeR\nv6ZpC78vA2l+UH9DtMolfC5Z55UAGfBr16XCc3qm1oeVADXFn3lyH0o5t4+H3+dsNqzyiVW5VDp2\n37HmFLHKFVYF0tbWVtQ1fv3rXwexG264gY5tbm6mcVahMjAwQMcWI3oeUSReWxuuAeyeWaMUZth3\n6O6+HkBYjyVS5TS3JWlG8xn6F83s1fw/W6fFBpnZHWa20cw2nuyLnAwoUlmKntuxU/ZEymmkC/o/\nAVgMYAWATgD/GBvo7qvdvd3d2yc28n+eiVSQEc3terLRSaTcRrSgu/shd8+6ew7APwO4srS3JTI+\nNLelmo1o67+ZzXb30/vF/wLAlnONPy3njlODYeIh7WHiCLWV27DinF4Pt9FPaYgcY9DTw+NTyPh6\nnricNmkWjdcdIc0zOl/kj7e0hYbTZMt9QyQJVoxcLHEU2fpeR+LTSJOIGnK/xRrx3M7l0NcXHsPA\nkqIsEVYNtmwJX4pJk3hjk+5u3pBl2rTwE6zYv25izSmOHDkSxDo6OujYpUuX0jgzcWLsWJHCsaMe\ngPixDizZyZKihR4LMezMMrOfArgewAwz6wDwTQDXm9kKDDWJ2Q3gLwt6NJEKorktSTPsgu7unyTh\nH47BvYiUlea2JI12ioqIJIQWdBGRhNCCLiKSEGVNt7s7MulwA0aqNvy54pEql/FrYfBO+x8OG3UA\nwB8f+V0Qa7+eV6JgaqRZBNlyH6tyaZnZRuP73/xDEGva+ls6dsZK9lEyMIVsvT4ZOcC/mPqADKn8\nAACLZPLTTioHWLXNODa4yOVy6O8PN86x7fKVXuUSa1qxevXqIPaRj3yEjmXVLABv1NBAKpYAYOHC\nhTTOtv6/8EJ41AUArFzJm7q0trYGMValVKxYg4tYlQqrimlsDKvcCq1y0Tt0EZGE0IIuIpIQWtBF\nRBJCC7qISEJURFK0vi68DSOxcXHgbRoe7OIJxhsunx0GZ13Ar52JHAkwgSRL+/lZza3L59F4+s0w\noblt62469v09PAE97YJlQWzitPl0bFyY4M3lIknRSE4zQ2JHT54IYtnIdcvB3WlCjG1rr5Tz0A8c\nOEDje/eSYyMA3HjjjUEslrhkRx4APOEXO6myvb2dxtkRBBs2bKBjP/WpT9H4vHnh3xuWKC1WbOs/\nSwYD/Jz0Y8eOBbHY63k2vUMXEUkILegiIgmhBV1EJCG0oIuIJIQWdBGRhChzlUsOaVLlUku6vXuk\nyqXsm7vn8Kz18X3/wsc3k23+J3mVy/a9B2l84sSwrqN5Im8KMa2R1YAAE1rD5gADJ8Ku9ACASJXL\nhQvfE8SOHD5Fx/JWBFyObeUHkItUAmRIpcyJU2EDhWykwqAc3J12jWdVDJVS5TJrFj+SYtu2bTTO\nKkNOnuRHYGzdupXGp06dGsQmT55Mx8aaZ8yZMyeIHTzI/y719vbS+KWXXhrE9u/fT8cWI1blEjvu\ngY0/fjz8e5rJ8L/nZ9M7dBGRhNCCLiKSEFrQRUQSQgu6iEhCDLugm9l8M3vKzLaa2etm9qV8fLqZ\nrTOzHfn/8wOQRSqU5rYkTSFVLhkAX3P3zWY2CcAmM1sH4LMAnnD3b5nZnQDuBPDXw13MSVaXVTcY\nOQNjXDz1JA2f6ODnXaAtrGjJneSZ77oJPIs/SF6jPQe66Nhp776QxmcsDrP46WefpmOxcQ+P/+vb\nw+tOjFTKRJGzXJyfS5GJnMWSJbVNfSTrnyMVJcMo2dx2d1qJUExDh3Jbt24dje/atYvGZ8wIa5li\nVS4TJ/KWJ+w1evPNN+nYq6++msaXL18exN544w069sUXX6Txj33sY0FsypRIw5kixKpcYmex1NSE\nFWbsbBtWLcUM+w7d3TvdfXP+1z0AtgGYC+AWAPfnh90P4NaCHlGkQmhuS9IUVYduZm0ArgDwPIBW\nd+/M/9FBAPSoMjO7A8AdADC5KXK6oMg4G+3cZu+0RMqt4KSomTUD+DmAL7v7O3Z1+NC/B+i/Cdx9\ntbu3u3t7Y0OFfIwicoZSzG0t6FIJClrQzawOQxP+x+7+cD58yMxm5/98NgD+Ia9IBdPcliQZ9iMX\nG8rq/BDANne/54w/WgPgMwC+lf//o8M+mvMP953t+K6QxFF2104anzUz0rTi6vcHoZ0v8KRjDdkG\nDQCHe8L14+3e8NB7AJjdNIHGW0lSNLUpbAwAAFu2HKLxKanwuU++jDTwABBPJ4Xf7xQ56gEAaiPJ\nUvPwfUc6EzaTKDRx9P+uW8q5HX+MIFYpSdEdO3bQ+IIFC2j8uuuuC2JPPsmLBqZPDxusAMCRI0eC\n2Ntv8yYy06bx4qIlS5YEsaamJjr22WefpXGWvFy5ciUd29LSQuNMMY0sYvHRJEUL+Qz9GgCfBvCa\nmb2cj30DQ5P9QTP7HIA9AMKSCJHKprktiTLsgu7uzyB+JtZNpb0dkfLR3Jak0U5REZGE0IIuIpIQ\nWtBFRBKirA0uAKfZZU+RnysVUtdbsy/MOAPA7NY2Gv/978Ks/8mjYeMDALik7l00fsWysELltxvW\n07Hb3+JbnqdPDutOahp5G4pL3033zQA3stqVsLqkWLHvbE1k23SKvO/IkMqXojf+l5C70+3drOqh\nUmrWOzo6aLytrY3GH3zwwSB2+PBhOvbyyy+ncbad/xe/+AUd+8orr9A4qzqJHTUQOz5g1apVND5W\niqnAih0fUAi9QxcRSQgt6CIiCaEFXUQkIbSgi4gkhBZ0EZGEKGuVi4Nne2ljgtoyF+AAOLEmjHV3\nhAfyA0DLCn4OS9ui8JyO7ukn6NjO7vBcCwDo2ROeoZLL8UqZpknNNH74RE8Qm9HIz595/L5/oPFr\nJm8PYo03fJaOjZ/mQs6qyPLXdCDWHIDEcvQAoPFFzykisbq6unLczjs8+mh4HM3OnfycotiZJpde\nGlZfsbNZgHj1y5Yt4XlCrOkFALS28uqrrq7wrKPYuS933303jbPzdD70oQ/RscVIp3kVWOyMF1bx\nRCsBS9XgQkREqoMWdBGRhNCCLiKSEFrQRUQSoryZR3dkWGMCumm7/Bu533rrfwex5XP5faQm8K3G\ntTVhm70prTxh2O3HafyPe/YHsbp63r6PbYEHgEwqfJ1XLOFb/3fv49fe8vzWIPaeG4ptIxgmg9JZ\nnvzMRE6yzZL3HewSRfa3KCl3H1VjgrG2adOmILZo0SI6NraNnh1tMH/+fDo2luhkSdFYw49itsDH\nErl79vDmMmvXrg1ipUiKxp53bRFFHtr6LyIiWtBFRJJCC7qISEJoQRcRSYhhF3Qzm29mT5nZVjN7\n3cy+lI//jZntN7OX8//dPPa3K1I6mtuSNIWkXjMAvubum81sEoBNZrYu/2ffdne+b5wYagIQZoHN\nSCXAmO7s5tuSe479nyCWmruYju1r4lvum6aGh++ne/rp2EUXTKDx+tSkILb/2EE+tikcC/AqFzTz\nLee9E3h871vh6/QeFFvlEvIcr/zIWaRih4zPRa5RpJLObVbhwKobYtvASyFWIcGaWcSaUEyYwOcl\n215/4gQ/1mLBggU0zo49OHDgAB3b3Mz/jjGTJ0+m8fpIddjrr78exErxfYm9/rFrs8ohFiu0WmrY\nBd3dOwF05n/dY2bbAMwt6OoiFUxzW5KmqM/QzawNwBUAns+Hvmhmr5rZvWbGT8cRqQKa25IEBS/o\nZtYM4OcAvuzu3QD+CcBiACsw9C7nHyNfd4eZbTSzjX2Do+9HKVJqpZjblbKBSM5vBS3oZlaHoQn/\nY3d/GADc/ZC7Z909B+CfAVzJvtbdV7t7u7u3N9aX/9hQkXMp1dwey8/FRQpVSJWLAfghgG3ufs8Z\n8dlnDPsLAOGeXpEKprktSVNIlcs1AD4N4DUzezkf+waAT5rZCgwdurIbwF8W8oAsC5yqLfO7myfX\n03DPsb4gZisX0rEDOf6vjbfDSyB7hFe5XDyNV6i0zp0VxAZ3kAsDqGuMZM9TJLs/hT+XCfOW0/i+\nF/4QBjeF57sAAFZey+N94XOPVblkIh9bsOHZHDkTqPiPPUo6t1l1QqzKYqysW7eOxlkjirlzef6X\nnUkD8IqWWIVK7IyXxYvDqrENGzbQsbEql1QqfB86ffp0OjZ2Xg1r+PHCCy/QsVdddRWNnzx5Moix\nOQDEz3hh/7KLXaMQhVS5PANeRPirET+qSAXQ3Jak0U5REZGE0IIuIpIQWtBFRBKi7A0uwDpak2YW\n2UiDi7BHdvF2vPgSjV9QSw72n80TidOO8C3PDT0DQWxfZw+/kTaeOLL+cMv9+2bxrc3InorEw6YB\nmQN87Lx5YTd3AGj+/ctBrHdTuGUaAJpjSdH+MClaE9kenYt8z9norJPt0fwOysLdC25MMJY16088\n8QSNNzU1BbFYUvTwYX40BksCdnZ20rHLl/NEe19fmNy/5JJL6NhYIpG9fvv3h01hAGDp0qU0/vDD\nDwex5557jo6NJUUHBsK/67E5UMz3nF2j0K/XO3QRkYTQgi4ikhBa0EVEEkILuohIQmhBFxFJiPJW\nuQC0yoVt1qupD6s0zo1lgXnGeckMfhrqzmPk59tGnvk+uJdveZ5YG25XtoNH6dht/8KrbTKZsBol\nk+HbsT0TZtoBIOWNQewEwkoHAKi9YCqNTxoIK3myr0e2/ke8ufm1IFYT2frvKX6MARudybLv7fie\neFholUtDQ7Fzu3BtbW00zrb+x7bc//GPf6Rx1vgiVl3ygx/8gMZZZUg6zU9hjVW5FDO2pSVsOBMb\n/+qrrxb8eADw9NNPFzW+UOzeVOUiInKe0YIuIpIQWtBFRBJCC7qISEJoQRcRSYiyVrk4gCypBMix\n9l1FNwYIr7t77a/pyD3beAMa7w+rOjo2PU9GAqkBnpmvGSTPr49XqOT6+DXSg+E5JZk0P/Q+S6s9\ngGwurKzpj2TK+/fw+2icEE6PZ7byKpfsX32DxjOLw2obi1SDWKTKxch9O7nEeLb1dPeCGxOUounF\nQw89ROPr1/PmLadOhZVTv/nNb+hYVokC8GoUdt1zxdm1Y48Xq1xhr3Pstd8ama8TJ4bnNsVeu5tv\nvpnGL7zwwiBWaKXTaax6ZTQNLvQOXUQkIbSgi4gkhBZ0EZGE0IIuIpIQwyZFzWwCgPUAGvLjH3L3\nb5rZIgAPALgAwCYAn3Z3nv17xwXDkBvJZkUSZHHhz6a2P/tzOrJtCu8mfurQ3iDWFEk6sm72AIDu\n3iCUIzEASIX9AoYufTJ8Ln29++jY45F4hrzQ0VfU+XM5hjB+weIldGzD4jYaf25v2BAjFctexhJK\nTtqa5EaeODqt1HObdXBnMda1vli33XYbjc+aNYvGY1v0mVhCs7u7O4gdP368qGv09ITNXo4dO0bH\nvv322zTOEonsdQbiSUqWiI015Yg14Ni8eXPBj1dM44uxbnAxAOBGd78cwAoAq8zsvQD+HsC33f0i\nAMcAfK6gRxSpHJrbkijDLug+5PRbzLr8fw7gRgCna6fuB3DrmNyhyBjR3JakKejffmZWY2YvA+gC\nsA7ATgDH3f10oWgHANqg0MzuMLONZraxf5DXO4uMl1LN7fLcrci5FbSgu3vW3VcAmAfgSgC88yr/\n2tXu3u7u7RPq60Z4myJjo1Rze8xuUKQIRWVn3P04gKcAvA/AVDM7nVSdB6DwrItIhdHcliQopMql\nBUDa3Y+bWSOAD2IoafQUgNswVA3wGQCPDnstADWkesVqw1g6UuUSf4/PxpPqCAC46noa5u0fRq/Y\nmobGzWF1yfbND9KxF0/nFQKpo2E8bE9wGq9IQD35iKyBN8PAxTNpuGbXM+G9sX37ADKxxhekcUWO\nZv2L2/tf0rlthpqacL7V1oaxQQJpAAAGrklEQVR/xWIVGaVw7bXXjtm1S+Gll8KmLmvXrqVj586l\nn3TR6pdY5VCsOoQdv8AaeADAsmXLaPyFF16gcSbWxIPdR6EVLUwhZ7nMBnC/mdVgaG160N1/aWZb\nATxgZncDeAnAD0d8FyLjQ3NbEmXYBd3dXwVwBYnvwtBnjiJVSXNbkkY7RUVEEkILuohIQpT1PHQA\nSLHt0bVhMqmOnFd8vjjxJ2FyZvqf/Fs6NpbIffE7fxdeI3OYjl3QwLf+n+wOk09TWyJ7CVbMpmF7\nILxGJFUNjxxO4OQrMllyRvo4nocO8MRcXV2Yxp80aVI5bqcise31sa31sSTlXXfdFcRiicQpU6bQ\n+OHD4d+FWBL2iiuCT+UAAH19fUGsFMc6sARqKbf+i4hIFdCCLiKSEFrQRUQSQgu6iEhCaEEXEUmI\nsla5mAENZEt/bR25jYljtRG/On3ve3zr/7v6eMXEJf3hluKJk3m3+XQ/byRQWxNWv2QHu+jYmoOv\n0PjAYHgEQTbHD3Coq+HT0chJAXXkmICx21A/PDOj2/zZ1m7Wcf589pWvfIXG2esJ8IqPmTP50ROx\nRhvs+AVWtQIAHR0dBV87m+WNV2LPhTWzYEdIFErv0EVEEkILuohIQmhBFxFJCC3oIiIJoQVdRCQh\nylvlAqCWNDeorSNZ3Zrz4WdNrKFD+Br9xy/cTse2Ra783N+8HMSaG/i3u26Q30eG3F+NR6bMwW4a\nzuXC720u0sgiEkaKxHuPhJU5uUwmHDjOWJVLrOLhfHXPPffQeGNjI41//vOfD2KxyqHubj4vWaVM\nrPHIwYMHC75GrMqFVbPEdHZ2BrFYg4yznQ+rpojIeUELuohIQmhBFxFJCC3oIiIJMWx2xswmAFgP\noCE//iF3/6aZ3QfgAwBO5Id+1t3DTNxZciTRxpKAIE0vkocnSlIIk3vf/c5P6Nhlp6bR+Lsz7JB8\nnjTMRn6sO03aRqbMYGybdrjN3xFJHEWSojmSrJowLXzeFjk6IKbUc7vQJgRKir7TF77wBRpnzUEA\nnmCMJR1jic5Cv1cAMDg4OOprFDO2paUliO3ataugry1kZg0AuNHde82sDsAzZvbr/J/9Z3d/qNAb\nFakwmtuSKMMu6D70o6U3/9u6/H/j3OxLZPQ0tyVpCvoM3cxqzOxlAF0A1rn78/k/+lsze9XMvm1m\nDZGvvcPMNprZxlODhdVSipRLqeZ2Mf+kFhkrBS3o7p519xUA5gG40swuBXAXgKUA3gNgOoC/jnzt\nandvd/f2pnr+mZjIeCnV3I59VitSTkVVubj7cQBPAVjl7p0+ZADA/wRw5VjcoEg5aG5LEthw/1Q0\nsxYAaXc/bmaNANYC+HsAm9y904bemnwbQL+73znMtQ4D2JP/7QwAR0b7BCqYnl/5LXT3sEQgYozm\ndiW+LqWW9OdYic+voLldSJXLbAD3m1kNht7RP+juvzSzJ/N/IQzAywD+argLnXlD+c8d2wt4/Kqk\n51cVSj63E/K6nFPSn2M1P79CqlxeBXAFid84JnckUiaa25I02ikqIpIQ47mgrx7Hxy4HPb/z0/nw\nuiT9OVbt8xs2KSoiItVBH7mIiCSEFnQRkYQo+4JuZqvM7A0ze9PMzlnbWy3M7F4z6zKzLWfEppvZ\nOjPbkf8/PxaxCpjZfDN7ysy2mtnrZvalfDwxz7EUNLerT9LmdlkX9Hy97/cA/DmAZQA+aWbLynkP\nY+Q+AKvOit0J4Al3XwLgifzvq1UGwNfcfRmA9wL4Qv77lqTnOCqa21UrUXO73O/QrwTwprvvcvdB\nAA8AuKXM91By7r4ewNGzwrcAuD//6/sB3FrWmyqh/Fb4zflf9wDYBmAuEvQcS0BzuwolbW6Xe0Gf\nC2DfGb/vyMeSqNXdT7fvPgigdTxvplTMrA1Dm3GeR0Kf4whpble5JMxtJUXLIH/udtXXh5pZM4Cf\nA/iyu3ef+WdJeY5SnKR835Myt8u9oO8HMP+M38/Lx5LokJnNBoD8/7vG+X5GJd/R5+cAfuzuD+fD\niXqOo6S5XaWSNLfLvaC/CGCJmS0ys3oAnwCwpsz3UC5rAHwm/+vPAHh0HO9lVPKnDv4QwDZ3v+eM\nP0rMcywBze0qlLS5XfadomZ2M4DvAKgBcK+7/21Zb2AMmNlPAVyPoWM3DwH4JoBfAHgQwAIMHat6\nu7ufnVyqCmZ2LYDfA3gN/7+z9Tcw9FljIp5jKWhuV5+kzW1t/RcRSQglRUVEEkILuohIQmhBFxFJ\nCC3oIiIJoQVdRCQhtKCLiCSEFnQRkYT4v0h9/B3FgoKMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8088d96f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show generated grey picture X_train[0]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[0])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(x, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Black and white pictures for all classes\n",
    "X_train_gray = np.array([color.rgb2gray(pic) for pic in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (39209,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train_zero.shape, X_train_gray.shape) #Q: What is the purpose of this line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pictures get edited in Preprocessing to generate a bigger amount of data\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(  #Adjust datagenerator\n",
    "    featurewise_center=True, #Set input mean to 0 over the dataset, feature-wise \n",
    "    #Q: What does this line mean?\n",
    "    featurewise_std_normalization=True, #Divide inputs by std of the dataset, feature-wise. \n",
    "    #Q: What does this line mean?\n",
    "    rotation_range=20, #Pictures get rotated (max. 20Â°)\n",
    "    width_shift_range=0.2, #Pictures get shifted horizontally (max. by the factor 0.2)\n",
    "    height_shift_range=0.2, #Pictures get shifted vertically (max. by the factor 0.2)\n",
    "    horizontal_flip=True) #Pictures get mirrored horizontally \n",
    "    #Q: Is this executed with all signs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 64, 64, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_zero = X_train_zero.reshape(len(X_train_zero), 64, 64, 1) # grayscale wrapper for the datagen object \n",
    "X_train_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "datagen.fit(X_train_zero) #Compute the internal data stats related to the \n",
    "#data-dependent transformations, based on an array of sample data.\n",
    "#Only required if featurewise_center or featurewise_std_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (36,27) (34,31) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-77fe68180f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_gray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_gray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Frage: Was macht diese Funktion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (36,27) (34,31) "
     ]
    }
   ],
   "source": [
    "X_train_gray.max(), X_train_gray.min() #Frage: Was macht diese Funktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color zero class\n",
    "since we have a colorfull zeroclass we do not need to transform the pictures to grey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load colorful signs\n",
    "#path = get_folderpath(subset='train', original_images=False)\n",
    "path=\"/data_on_server/Images/GTSRB_64x64/Final_Training\"\n",
    "X_train, y_train = load_data(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load colorful zero classes\n",
    "#path_zero = \"Y:\\\\Zero_Class_color\\\\00043\"\n",
    "path_zero = \"//data_on_server//Images//Zero_Class_color\"\n",
    "X_train_zero, y_train_zero = load_data(path_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The whole dataset consists of the zeroclasses as well as the normal signs\n",
    "X_trainC=X_train+X_train_zero\n",
    "Y_trainC=y_train+y_train_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the trainingimages in a numpy array\n",
    "sign=np.array(Y_trainC)\n",
    "unique_classes = len(np.unique(sign))\n",
    "y_trainN = np.eye(unique_classes)[np.array(sign, dtype=int)]\n",
    "X_trainN = np.array(X_trainC, dtype=np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Testdata:\n",
    "path=\"/data_on_server/Images/GTSRB_64x64/Final_Test\"\n",
    "X_test, Y_test = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the testimages in a numpy array\n",
    "sign_test=np.array(Y_test)\n",
    "unique_classes_test = len(np.unique(sign_test))+1\n",
    "Y_testN = np.eye(unique_classes_test)[np.array(sign_test, dtype=int)]\n",
    "X_testN = np.array(X_test, dtype=np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify all Trainingimages which have not the specified format\n",
    "for n in X_trainN:\n",
    "    if(n.shape != (64,64,3)):\n",
    "        print(n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52437, 44)\n",
      "(52437, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#Number of training images\n",
    "print (np.array(y_trainN).shape)\n",
    "print (X_trainN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 644345856 into shape (48778,64,64,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-e0a929a45d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#grayscale wrapper for the datagen object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_trainN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 644345856 into shape (48778,64,64,3)"
     ]
    }
   ],
   "source": [
    "#grayscale wrapper for the datagen object #Q: What should be done here?\n",
    "X_train = X_trainN.reshape(len(X_train), 64, 64, 3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize X_train\n",
    "X_train = np.array(X_train, dtype=np.float32) / 255\n",
    "# X_train_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the labels\n",
    "boundBox=np.array(y_train)[:,0:4]\n",
    "sign=np.array(y_train)[:,4]\n",
    "unique_classes = len(np.unique(sign))\n",
    "y_trainN = np.eye(unique_classes)[np.array(sign, dtype=int)]\n",
    "y_trainF=np.concatenate([y_trainN,boundBox], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network:\n",
    "From Keras docs, VGG-like Convnet.\n",
    "\n",
    "First Block (Convolutional Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Keras Model libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 2D convolution layer\n",
    "model.add(Conv2D(input_shape=(64, 64, 3), # Specifying the input shape\n",
    "                 filters=32, #Number of filters\n",
    "                 kernel_size=(3, 3), #Filtersize (3x3 filter)\n",
    "                 activation='relu')) #Activation function is Rectifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 2D convolution layer\n",
    "model.add(Conv2D(filters=32,\n",
    "                 kernel_size=(3, 3),\n",
    "                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add MaxPooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #poolsize 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applies Dropout to the input. Dropout consists in randomly setting a fraction rate (here 0.25) of input units to 0\n",
    "# at each update during training time, which helps prevent overfitting.\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Block (Convolutional Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 2D convolution layer\n",
    "model.add(Conv2D(filters=64, #Number of filters\n",
    "                 kernel_size=(3, 3), #Filtersize (3x3 filter)\n",
    "                 activation='relu')) #Activation function is Rectifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 2D convolution layer\n",
    "model.add(Conv2D(filters=64, #Number of filters\n",
    "                 kernel_size=(3, 3), #Filtersize (3x3 filter)\n",
    "                 activation='relu')) #Activation function is Rectifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add MaxPooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #poolsize 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applies Dropout to the input. Dropout consists in randomly setting a fraction rate (here 0.25) of input units to 0\n",
    "# at each update during training time, which helps prevent overfitting.\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Block (Fully Connected Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten the input --> From 2D-Array to 1D-Array\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add fullyconnected layers\n",
    "model.add(Dense(200, activation='relu')) #Add layer with 200 neurons with rectifier activation function\n",
    "model.add(Dropout(0.5)) #Applies Dropout to the input --> fraction rate is here 0.5\n",
    "model.add(Dense(44, activation='softmax')) #Add layer with 44 neurons with softmax activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define critical parameters for network (SGD-optimizer, tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic gradient descent optimizer.\n",
    "#Includes support for momentum, learning rate decay, and Nesterov momentum.\n",
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.01, #Learningrate = 0.01, \n",
    "          decay=1e-6, #decay (Factor to decrease the learningrate over time),\n",
    "          momentum=0.9, #momentum (former gradient descents are considered to increase the speed of convergence)= 0.9\n",
    "          nesterov=True) #Nesterov momentum is used (method to reduce the probability to not find minima because of high momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', #As lossfunction y_true and y_pred are compared in every category\n",
    "              optimizer=sgd, #Stochastic gradient descent optimizers are used\n",
    "              metrics=['accuracy']) #List of metrics to be evaluated by the model during training and testing --> Here: accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', #Path of directory where to save the log files to be parsed by TensorBoard.\n",
    "                          histogram_freq=0, #frequency (in epochs) at which to compute activation and weight histograms\n",
    "                                            #If set to 0, histograms won't be computed.\n",
    "                          write_graph=True, #Visualize the graph in TensorBoard.\n",
    "                          write_images=False) #Write model weights to visualize as image in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other Callbacks: #Q: Where are they used?\n",
    "from keras.callbacks import Callback\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
    "\n",
    "#Q: What is this/ Is this training function used?\n",
    "model.fit(X_trainN, y_trainN, batch_size=32, epochs=50, validation_data=(X_testN, Y_testN), shuffle=True, callbacks=[TestCallback((X_testN, Y_testN))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52437 samples, validate on 12630 samples\n",
      "Epoch 1/50\n",
      "52437/52437 [==============================] - 304s - loss: 1.1202 - acc: 0.6715 - val_loss: 0.3015 - val_acc: 0.9137\n",
      "Epoch 2/50\n",
      "52437/52437 [==============================] - 304s - loss: 0.2011 - acc: 0.9393 - val_loss: 0.2061 - val_acc: 0.9428\n",
      "Epoch 3/50\n",
      "52437/52437 [==============================] - 304s - loss: 0.1113 - acc: 0.9662 - val_loss: 0.1372 - val_acc: 0.9638\n",
      "Epoch 4/50\n",
      "52437/52437 [==============================] - 304s - loss: 0.0793 - acc: 0.9760 - val_loss: 0.1240 - val_acc: 0.9689\n",
      "Epoch 5/50\n",
      "52437/52437 [==============================] - 303s - loss: 0.0584 - acc: 0.9820 - val_loss: 0.1496 - val_acc: 0.9644\n",
      "Epoch 6/50\n",
      "52437/52437 [==============================] - 304s - loss: 0.0503 - acc: 0.9845 - val_loss: 0.1107 - val_acc: 0.9713\n",
      "Epoch 7/50\n",
      "22752/52437 [============>.................] - ETA: 161s - loss: 0.0426 - acc: 0.9871"
     ]
    }
   ],
   "source": [
    "#Train model with tensorborad\n",
    "model.fit(X_trainN, y_trainN, batch_size=32, epochs=50, validation_data=(X_testN, Y_testN), shuffle=True, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model with TestCallback\n",
    "#Q: Will this be done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training without tensorboard\n",
    "model.fit(X_trainN, y_trainN, batch_size=32, verbose=2 ,epochs=20)\n",
    "#Q: What is the benefit of training with and without tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "model.save('16-10-17_convnet_Zero_color_E50.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
